{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QFlovoYUNCFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install duckdb\n",
        "!pip install astropy\n",
        "!pip install astroquery\n",
        "!pip install aiohttp\n",
        "!pip install aiofiles\n",
        "!pip install xmltodict\n",
        "!pip install tqdm\n",
        "%cd /content/drive/MyDrive/spaceapps"
      ],
      "metadata": {
        "id": "ktnQSGHe9ENW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# async version\n",
        "import asyncio\n",
        "import aiofiles\n",
        "from aiohttp import ClientSession, CookieJar, ClientTimeout\n",
        "import xmltodict\n",
        "\n",
        "\n",
        "total_rows = 1_811_709_771\n",
        "active_jobs = []\n",
        "retrieved_rows = 0\n",
        "retrieved_batches = []\n",
        "batch_siz = 50_000_000\n",
        "\n",
        "\n",
        "\n",
        "class GaiaQueryJob:\n",
        "  start_job_url = 'https://gea.esac.esa.int/tap-server/tap/async'\n",
        "  query = \"\"\"\n",
        "with gaia_ap as (\n",
        "select source_id, mg_gspphot from gaiadr3.astrophysical_parameters where mg_gspphot is not null\n",
        "),\n",
        "gs as (\n",
        "\tselect source_id, ra, dec, distance_gspphot from gaiadr3.gaia_source where ra is not null and dec is not null and distance_gspphot is not null and random_index >= {0} and random_index < {1}\n",
        ")\n",
        "\n",
        "\n",
        "select gs.source_id, gs.ra, gs.dec, gs.distance_gspphot, gaia_ap.mg_gspphot from gs join gaia_ap on gs.source_id = gaia_ap.source_id\n",
        "\"\"\"\n",
        "\n",
        "  def __init__(self, start, end, session):\n",
        "    self.start = start\n",
        "    self.end = end\n",
        "    self.session = session\n",
        "    self.jobId = None\n",
        "    self.phase = 'PENDING'\n",
        "\n",
        "  async def launch_job(self):\n",
        "    data = {\n",
        "        'PHASE': 'RUN',\n",
        "        'LANG': 'ADQL',\n",
        "        'REQUEST': 'doQuery',\n",
        "        'FORMAT': 'csv',\n",
        "        'QUERY': GaiaQueryJob.query.format(self.start, self.end)\n",
        "    }\n",
        "    async with self.session.post(GaiaQueryJob.start_job_url, data=data) as req:\n",
        "        res = xmltodict.parse(await req.text())['uws:job']\n",
        "        self.jobId = res['uws:jobId']\n",
        "        self.phase = res.get('uws:phase', self.phase)\n",
        "\n",
        "  async def get_phase(self):\n",
        "    if self.jobId == None: return\n",
        "    async with self.session.get(f'{GaiaQueryJob.start_job_url}/{self.jobId}') as req:\n",
        "        res = xmltodict.parse(await req.text())['uws:job']\n",
        "        self.phase = res.get('uws:phase', self.phase)\n",
        "\n",
        "  async def run(self):\n",
        "    try:\n",
        "      await self.launch_job()\n",
        "      print(f'Job launched: {self.jobId}')\n",
        "    except Exception as e:\n",
        "      print(f'Job launch failed: {e}')\n",
        "\n",
        "    try:\n",
        "      while self.phase != 'ABORTED' and self.phase !='ERROR' and self.phase != 'COMPLETED':\n",
        "        await asyncio.sleep(60)\n",
        "        await self.get_phase()\n",
        "    except Exception as e:\n",
        "      print(f'Failed to fetch phase: {e}')\n",
        "\n",
        "    if self.phase == 'ABORTED' or self.phase == 'ERROR':\n",
        "      print(f'Job {self.jobId} failed: {self.phase}')\n",
        "      return\n",
        "\n",
        "    print(f'Job {self.jobId} completed')\n",
        "\n",
        "    async with self.session.get(f'{GaiaQueryJob.start_job_url}/{self.jobId}/results/result') as req:\n",
        "      async with aiofiles.open(f'gaia_data/{self.start}-{self.end}.csv', 'wb+') as f:\n",
        "        async for data in req.content.iter_chunked(16384):\n",
        "          await f.write(data)\n",
        "\n",
        "    print(f'Job {self.jobId} results saved')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "async def get_tables(session):\n",
        "  async with session.get('https://gea.esac.esa.int/tap-server/tap/jobs/async') as req:\n",
        "    jobs = xmltodict.parse(await req.text())['uws:jobs']['uws:job']\n",
        "    jobs = list(map(lambda x: x['uws:jobId'], jobs))\n",
        "    assert len(jobs) == 37\n",
        "  return jobs\n",
        "\n",
        "async def download(jobId, session):\n",
        "  async with session.get(f'https://gea.esac.esa.int/tap-server/tap/async/{jobId}/results/result', timeout=None) as req:\n",
        "      async with aiofiles.open(f'gaia_data/{jobId}.csv', 'wb+') as f:\n",
        "        async for data in req.content.iter_chunked(16384):\n",
        "          await f.write(data)\n",
        "\n",
        "async def bulk_download(jobs, session):\n",
        "  tasks = []\n",
        "  for job in jobs:\n",
        "    tasks.append(download(job, session))\n",
        "  await asyncio.gather(*tasks)\n",
        "\n",
        "no_timeout = ClientTimeout(\n",
        "    total=None\n",
        ")\n",
        "\n",
        "async def main():\n",
        "  async with ClientSession(cookie_jar=CookieJar(), timeout=no_timeout) as session:\n",
        "    await session.post('https://gea.esac.esa.int/tap-server/login', data={}) #username passowrd here\n",
        "    print('Logged into Gaia')\n",
        "    jobs = await get_tables(session)\n",
        "    await bulk_download(jobs, session)\n",
        "    # tasks = []\n",
        "    # for i in range(0, total_rows, batch_siz):\n",
        "    #   tasks.append(GaiaQueryJob(i, i+batch_siz, session).run())\n",
        "    # await asyncio.gather(*tasks)\n",
        "\n",
        "await main()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EqoJqnOjWJu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sync version\n",
        "from astroquery.gaia import Gaia\n",
        "import json\n",
        "import time\n",
        "\n",
        "total_rows = 1_811_709_771\n",
        "active_jobs = []\n",
        "retrieved_rows = 0\n",
        "retrieved_batches = []\n",
        "batch_siz = 50_000_000\n",
        "max_active_jobs = 3\n",
        "query = \"\"\"\n",
        "with gaia_ap as (\n",
        "select source_id, mg_gspphot from gaiadr3.astrophysical_parameters where mg_gspphot is not null\n",
        "),\n",
        "gs as (\n",
        "\tselect source_id, ra, dec, distance_gspphot from gaiadr3.gaia_source where ra is not null and dec is not null and distance_gspphot is not null and random_index >= {0} and random_index < {1}\n",
        ")\n",
        "\n",
        "\n",
        "select gs.source_id, gs.ra, gs.dec, gs.distance_gspphot, gaia_ap.mg_gspphot from gs join gaia_ap on gs.source_id = gaia_ap.source_id\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def pick_next_batch():\n",
        "  global active_jobs, retrieved_rows, retrieved_batches, batch_siz\n",
        "  active_job_idx = list(map(lambda x: x[1], active_jobs))\n",
        "  for i in range(0, total_rows, batch_siz):\n",
        "    if i not in retrieved_batches and i not in active_job_idx:\n",
        "      return i\n",
        "\n",
        "def launch_new_job():\n",
        "  global active_jobs, retrieved_rows, retrieved_batches, batch_siz\n",
        "  # job variable fields: https://github.com/astropy/astroquery/blob/e8fff0b51bdc2f367b21c8564195599aafca19fd/astroquery/utils/tap/model/job.py#L52\n",
        "  batch_idx = pick_next_batch()\n",
        "  job = Gaia.launch_job_async(query.format(batch_idx, batch_idx+batch_siz), dump_to_file = True, output_file = f'gaia_data/{batch_idx}-{batch_idx+batch_siz-1}.csv', output_format = 'csv', background = True)\n",
        "  active_jobs.append((job, batch_idx))\n",
        "  print(f'Started job: {job.jobid}')\n",
        "\n",
        "def check_active_jobs():\n",
        "  global active_jobs, retrieved_rows, retrieved_batches, batch_siz\n",
        "  remove_idx = []\n",
        "  for idx, (job, batch_idx) in enumerate(active_jobs):\n",
        "    try:\n",
        "      job.get_phase(update=True)\n",
        "      if not job.is_finished():\n",
        "        print(f'Job {job.jobid} in phase: {job.get_phase()}')\n",
        "        continue\n",
        "      if (job.get_phase() != 'COMPLETED'):\n",
        "        remove_idx.append(idx)\n",
        "        continue\n",
        "      job.save_results()\n",
        "      retrieved_rows += batch_siz\n",
        "      retrieved_batches.append(batch_idx)\n",
        "      with open('checkpoint.json', 'w+') as f:\n",
        "        json.dump([retrieved_batches, retrieved_rows], f)\n",
        "      Gaia.remove_jobs(job.jobid)\n",
        "      remove_idx.append(idx)\n",
        "    except:\n",
        "      print(f'Failed to check job: {job.jobid}')\n",
        "  print(f'Current Progress: {(retrieved_rows/total_rows):.2f} {retrieved_rows}/{total_rows}')\n",
        "  active_jobs = [ele for idx,ele in enumerate(active_jobs) if idx not in remove_idx]\n",
        "\n",
        "def load_checkpoint():\n",
        "  global active_jobs, retrieved_rows, retrieved_batches, batch_siz\n",
        "  try:\n",
        "    with open('checkpoint.json', 'r') as f:\n",
        "      retrieved_batches, retrieved_rows = json.load(f)\n",
        "  except:\n",
        "    print(f'Failed to load checkpoint')\n",
        "\n",
        "\n",
        "Gaia.login() #username and password here\n",
        "\n",
        "load_checkpoint()\n",
        "\n",
        "while retrieved_rows < total_rows:\n",
        "  check_active_jobs()\n",
        "  while (retrieved_rows < total_rows and len(active_jobs)<max_active_jobs and pick_next_batch() != None):\n",
        "    launch_new_job()\n",
        "  time.sleep(120)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AAXekvh4Hupo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import duckdb\n",
        "from astropy import units as u\n",
        "from astropy.coordinates.sky_coordinate import SkyCoord\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "import math\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "\n",
        "def celestial_to_cartesian(ra, dec, dis):\n",
        "    x = dis*math.cos(math.radians(dec))*math.cos(math.radians(ra))\n",
        "    y = dis*math.cos(math.radians(dec))*math.sin(math.radians(ra))\n",
        "    z = dis*math.sin(math.radians(dec))\n",
        "    return [x, y, z]\n",
        "\n",
        "planets_all = duckdb.sql(\"select pl_name, ra, dec, sy_dist from 'exoplanets.csv'\").fetchall()\n",
        "planets_all = list(map(lambda x: (x[0], *celestial_to_cartesian(x[1],x[2],x[3])), filter(lambda x: x[3] != None, planets_all)))\n",
        "gaia_files = os.listdir('gaia_data')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5PW9gJo3523J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (pl_name, _, _, _) in tqdm(planets_all):\n",
        "  if (not os.path.exists(f'staged_data/{pl_name}')):\n",
        "    os.mkdir(f'staged_data/{pl_name}')\n",
        "    print(f'Created dir {pl_name}')\n"
      ],
      "metadata": {
        "id": "xQQBG-6saf3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gpu_kernel = cp.RawKernel(r'''\n",
        "extern \"C\" __global__\n",
        "void my_kernel(const int N, const float* d, const float* de, const float* ra, const float* mg, const float* base, float* y) {\n",
        "  int idx = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "  if (idx < N) {\n",
        "      float sra, cra, sde, cde;\n",
        "      sincosf(0.017453292 * ra[idx], &sra, &cra);\n",
        "      sincosf(0.017453292 * de[idx], &sde, &cde);\n",
        "      y[idx] = mg[idx]+2.5*log10f(powf(d[idx]*cra*cde-base[0], 2)+powf(d[idx]*cde*sra-base[1], 2)+powf(d[idx]*sde, 2))-5;\n",
        "  }\n",
        "}\n",
        "''', 'my_kernel')\n",
        "\n",
        "async def main():\n",
        "  # folder level checkpoint\n",
        "  checkpoint = []\n",
        "  try:\n",
        "    with open('checkpoint.json', 'r') as f:\n",
        "      checkpoint = json.load(f)\n",
        "  except:\n",
        "    pass\n",
        "  for gaia_file in gaia_files:\n",
        "    if gaia_file in checkpoint: continue\n",
        "    stars_all = duckdb.sql(f\"select source_id as id,ra,dec,distance_gspphot as dist,mg_gspphot as mag from 'gaia_data/{gaia_file}'\").fetchnumpy()\n",
        "    n = stars_all['ra'].shape[0]\n",
        "    d = cp.asarray(np.float32(stars_all['dist']))\n",
        "    de = cp.asarray(np.float32(stars_all['dec']))\n",
        "    ra = cp.asarray(np.float32(stars_all['ra']))\n",
        "    mg = cp.asarray(np.float32(stars_all['mag']))\n",
        "    blk_siz = 128\n",
        "    grid_siz = math.floor((n+blk_siz-1)/blk_siz)\n",
        "    stars_map = {}\n",
        "    tot = 0\n",
        "    with zipfile.ZipFile(f'staged_data/{gaia_file}.zip', 'w') as zf:\n",
        "      for (pl_name, px, py, pz) in tqdm(planets_all):\n",
        "        # file level checkpoint\n",
        "        # if os.path.exists(f'staged_data/{pl_name}/{gaia_file}.json'): continue\n",
        "        base = cp.array([px, py, pz], dtype=cp.float32)\n",
        "        res_vec = cp.zeros((n, ), dtype=cp.float32)\n",
        "        gpu_kernel((grid_siz, ), (blk_siz, ), (n, d, de, ra, mg, base, res_vec))\n",
        "        res = res_vec.get()\n",
        "        mask = np.where(res<=6)[0]\n",
        "        out = [[int(stars_all['id'][idx]), x - px, y - py, z - pz, np.float64(res[idx])] for idx in mask for x, y, z in [celestial_to_cartesian(stars_all['ra'][idx], stars_all['dec'][idx], stars_all['dist'][idx])]]\n",
        "        zf.writestr(f'{pl_name}.json', json.dumps(out))\n",
        "    checkpoint.append(gaia_file)\n",
        "    with open('checkpoint.json', 'w+') as f:\n",
        "      json.dump(checkpoint, f)\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "id": "aIv50oUhM1Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for gaia_file in gaia_files[:1]:\n",
        "  stars_all = duckdb.sql(f\"select source_id as id,ra,dec,distance_gspphot as dist,mg_gspphot as mag from 'gaia_data/{gaia_file}'\").fetchnumpy()\n"
      ],
      "metadata": {
        "id": "vDdzBEkloE3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from cupyx.profiler import benchmark\n",
        "\n",
        "gpu_kernel = cp.RawKernel(r'''\n",
        "extern \"C\" __global__\n",
        "void my_kernel(const int N, const float* d, const float* de, const float* ra, const float* mg, const float* base, float* y) {\n",
        "  int idx = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "  if (idx < N) {\n",
        "      float sra, cra, sde, cde;\n",
        "      sincosf(0.017453292 * ra[idx], &sra, &cra);\n",
        "      sincosf(0.017453292 * de[idx], &sde, &cde);\n",
        "      y[idx] = mg[idx]+2.5*log10f(powf(d[idx]*cra*cde-base[0], 2)+powf(d[idx]*cde*sra-base[1], 2)+powf(d[idx]*sde, 2))-5;\n",
        "  }\n",
        "}\n",
        "''', 'my_kernel')\n",
        "\n",
        "def f(stars_all, planets_all, n, d, de, ra, mg):\n",
        "  blk_siz = 128\n",
        "  grid_siz = math.floor((n+blk_siz-1)/blk_siz)\n",
        "  stars_map = {}\n",
        "  tot = 0\n",
        "  for (pl_name, px, py, pz) in planets_all[:1]:\n",
        "    base = cp.array([px, py, pz], dtype=cp.float32)\n",
        "    res_vec = cp.zeros((n, ), dtype=cp.float32)\n",
        "    gpu_kernel((grid_siz, ), (blk_siz, ), (n, d, de, ra, mg, base, res_vec))\n",
        "    res = res_vec.get()\n",
        "    mask = np.where(res<=6)[0]\n",
        "    out = [[int(stars_all['id'][idx]), *celestial_to_cartesian(stars_all['ra'][idx], stars_all['dec'][idx], stars_all['dist'][idx]), np.float64(res[idx])] for idx in mask]\n",
        "    with open(f'exoplanet_stars_map/{pl_name}.json', 'r+') as f:\n",
        "      temp = json.load(f)\n",
        "      temp += out\n",
        "      f.seek(0)\n",
        "      json.dump(temp, f)\n",
        "      f.truncate()\n",
        "\n",
        "n = stars_all['ra'].shape[0]\n",
        "d = cp.asarray(np.float32(stars_all['dist']))\n",
        "de = cp.asarray(np.float32(stars_all['dec']))\n",
        "ra = cp.asarray(np.float32(stars_all['ra']))\n",
        "mg = cp.asarray(np.float32(stars_all['mag']))\n",
        "\n",
        "print(benchmark(f, (stars_all, planets_all, n, d, de, ra, mg), n_repeat=100))\n"
      ],
      "metadata": {
        "id": "LP1h0EwXp8aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "f = lambda x: x[3]+2.5*math.log10(base[2]*base[2] + x[2]*x[2] - 2*base[2]*x[2]*(math.sin(base[1])*math.sin(x[1])+math.cos(base[1])*math.cos(x[1])*math.cos(x[0]-base[2])))\n",
        "\n",
        "res_classic = np.apply_along_axis(f, 1, stars)\n",
        "\n",
        "print(res_classic)"
      ],
      "metadata": {
        "id": "VY0r6wj6LkZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stars_vec = np.column_stack((d*d, d*np.sin(de), d*np.cos(de)*np.cos(ra - base[0])))\n",
        "res_vec = np.add(mg, 2.5*np.log10(np.dot(stars_vec,np.array([1, -2*base[2]*math.sin(base[1]), -2*base[2]*math.cos(base[1])])) + (base[2]*base[2])))\n",
        "\n",
        "res_vec"
      ],
      "metadata": {
        "id": "rdr_iAicRMKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(np.min(res_vec))\n",
        "print((res_vec < 11).sum())\n",
        "\n",
        "plt.hist(res_vec, bins=10)"
      ],
      "metadata": {
        "id": "2lqJURKG3Tse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "gbase = cp.array([base[0], base[1], base[2]]);\n",
        "gd = cp.asarray(stars_all['dist'])\n",
        "gde = cp.asarray(stars_all['dec'])\n",
        "gra = cp.asarray(stars_all['ra'])\n",
        "gmg = cp.asarray(stars_all['mag'])\n",
        "\n",
        "stars_vec = cp.column_stack((gd*gd, gd*cp.sin(gde), gd*cp.cos(gde)*cp.cos(gra - gbase[0])))\n",
        "res_vec = cp.add(gmg, 2.5*cp.log10(cp.dot(stars_vec,cp.array([1, -2*base[2]*math.sin(base[1]), -2*base[2]*math.cos(base[1])]))))\n",
        "cp.asnumpy(res_vec)"
      ],
      "metadata": {
        "id": "ivyuOWwsXt0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "gpu_kernel = cp.RawKernel(r'''\n",
        "extern \"C\" __global__\n",
        "void my_kernel(const float* d, const float* de, const float* ra, const float* mg, const float* base, float* y) {\n",
        "  int idx = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "  y[idx] = mg[idx]+2.5*log10f(base[2]*base[2] + d[idx]*d[idx] - 2*base[2]*d[idx]*(sinf(base[1])*sinf(de[idx])+cosf(base[1])*cosf(de[idx])*cosf(ra[idx]-base[2])));\n",
        "}\n",
        "''', 'my_kernel')\n",
        "\n",
        "gbase = cp.array([base[0], base[1], base[2]], dtype=cp.float32);\n",
        "gd = cp.asarray(d)\n",
        "gde = cp.asarray(de)\n",
        "gra = cp.asarray(ra)\n",
        "gmg = cp.asarray(mg)\n",
        "res_vec = cp.zeros((1000000, ), dtype=cp.float32)\n",
        "\n",
        "gpu_kernel((50000, ), (20, ), (gd, gde, gra, gmg, gbase, res_vec))\n",
        "\n"
      ],
      "metadata": {
        "id": "aeX3qlGIuJ8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import json\n",
        "import tqdm\n",
        "import duckdb\n",
        "\n",
        "planets_all = duckdb.sql(\"select pl_name, ra, dec, sy_dist from 'exoplanets.csv'\").fetchall()\n",
        "planets_all = list(map(lambda x: x[0], filter(lambda x: x[3] != None, planets_all)))\n",
        "\n",
        "zfs = []\n",
        "\n",
        "for staged_file in os.listdir('staged_data'):\n",
        "  zfs.append(zipfile.ZipFile(f'staged_data/{staged_file}', 'r'))\n",
        "\n",
        "\n",
        "with zipfile.ZipFile('test.zip', 'w') as final:\n",
        "  for pl_name in tqdm.tqdm(planets_all[:3]):\n",
        "    stars = []\n",
        "    for zf in zfs:\n",
        "      stars += json.loads(zf.read(f'{pl_name}.json').decode('utf-8'))\n",
        "    final.writestr(f'{pl_name}.json', json.dumps(stars))\n",
        "\n"
      ],
      "metadata": {
        "id": "_hSmZ4icjoht"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}